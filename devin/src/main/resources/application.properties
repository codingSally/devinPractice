# Server configuration
server.port=8081
server.compression.enabled=true
server.compression.mime-types=application/json,application/xml,text/html,text/xml,text/plain,text/css,application/javascript
server.compression.min-response-size=1024
server.tomcat.max-threads=200
server.tomcat.max-connections=10000
server.tomcat.accept-count=100
server.tomcat.connection-timeout=5000

# Database configuration
spring.datasource.url=jdbc:h2:mem:calligraphy_db
spring.datasource.driverClassName=org.h2.Driver
spring.datasource.username=sa
spring.datasource.password=
spring.h2.console.enabled=true
spring.h2.console.path=/h2-console

# Connection pool configuration
spring.datasource.hikari.maximum-pool-size=10
spring.datasource.hikari.minimum-idle=5
spring.datasource.hikari.idle-timeout=30000
spring.datasource.hikari.connection-timeout=10000
spring.datasource.hikari.max-lifetime=1800000

# MyBatis configuration
mybatis.mapper-locations=classpath:mapper/*.xml
mybatis.type-aliases-package=com.example.devin.model
mybatis.configuration.cache-enabled=true
mybatis.configuration.lazy-loading-enabled=true
mybatis.configuration.aggressive-lazy-loading=false
mybatis.configuration.default-statement-timeout=30

# JWT configuration
jwt.secret=${JWT_SECRET:${random.uuid}}
jwt.expiration=86400000

# Initialize schema and data
spring.sql.init.mode=always
spring.sql.init.schema-locations=classpath:sql/schema.sql

# Redis configuration
spring.redis.host=${REDIS_HOST:localhost}
spring.redis.port=${REDIS_PORT:6379}
spring.redis.timeout=2000
spring.cache.type=redis
spring.cache.redis.time-to-live=600000
spring.cache.redis.cache-null-values=false

# Performance optimization
spring.mvc.async.request-timeout=30000
spring.resources.cache.period=3600
spring.resources.chain.enabled=true
spring.resources.chain.strategy.content.enabled=true
spring.resources.chain.strategy.content.paths=/**

# LLM API Configuration
# For local Ollama deployment (recommended for China)
llm.api.url=${LLM_API_URL:http://localhost:11434/v1/chat/completions}
llm.api.key=${LLM_API_KEY:}
llm.api.model=${LLM_API_MODEL:llama2}

# Alternative configurations (may not work in China):
# ChatAnywhere: llm.api.url=https://api.chatanywhere.com.cn/v1/chat/completions
